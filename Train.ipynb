{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from transformers import *\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from util_loss import ResampleLoss\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,BertConfig\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.path_base='/nfs/SSD-Data/winworkspace/HacktonMDA/'\n",
    "        self.SEED =42\n",
    "        self.trainset=os.path.join(self.path_base,'MDA---Hackathon','training_dict_219_new.pickle')\n",
    "        self.out_trainset= os.path.join(self.path_base,'dataset','Training_dataset.xlsx')\n",
    "        self.source_dir = os.path.join(self.path_base,'Model')\n",
    "        self.prefix = 'pubmed' \n",
    "        self.suffix = 'gt2020.rand123'\n",
    "        self.model_name = 'biobert_base'\n",
    "        self.loss_func_name = 'BCE'\n",
    "\n",
    "        self.max_len = 512\n",
    "        self.lr = 4e-4\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def __getattribute__(self, name):\n",
    "        return object.__getattribute__(self, name)\n",
    "    \n",
    "config=Config()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Report_quality</th>\n",
       "      <th>ภาพรวม</th>\n",
       "      <th>สภาวะตลาด</th>\n",
       "      <th>สรุปการผลการดำเนินงาน</th>\n",
       "      <th>เหตุการณ์สำคัญ</th>\n",
       "      <th>ปัจจัยต่อการเติบโต</th>\n",
       "      <th>เปิดเผยรายได้พร้อมคำอธิบาย</th>\n",
       "      <th>เปิดเผยกำไรที่เปลี่ยนไปพร้อมคำอธิบาย</th>\n",
       "      <th>เปิดเผยรายได้</th>\n",
       "      <th>เปิดเผยกำไรที่เปลี่ยนไป</th>\n",
       "      <th>วิเคราะห์งบหรือฐานะทางการเงิน</th>\n",
       "      <th>รายงาน ESG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>ADVANC_2021048695</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>2S_2021057723</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>3BBIF_2021052508</td>\n",
       "      <td>B</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>AMC_2021059653</td>\n",
       "      <td>B</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>BAM_2021055803</td>\n",
       "      <td>B</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>DCON_84847201</td>\n",
       "      <td>B</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>DRT_84296701</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>GBX_84528501</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>KKC_84909601</td>\n",
       "      <td>C</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>MNIT_84364001</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Quarter           Filename Report_quality ภาพรวม สภาวะตลาด  \\\n",
       "0    2021        1  ADVANC_2021048695              A      Y         Y   \n",
       "1    2021        1      2S_2021057723              C      N         N   \n",
       "2    2021        1   3BBIF_2021052508              B      N         N   \n",
       "3    2021        1     AMC_2021059653              B      N         N   \n",
       "4    2021        1     BAM_2021055803              B      N         N   \n",
       "..    ...      ...                ...            ...    ...       ...   \n",
       "215  2023        3      DCON_84847201              B      Y         N   \n",
       "216  2023        3       DRT_84296701              A      Y         Y   \n",
       "217  2023        3       GBX_84528501              C      N         N   \n",
       "218  2023        3       KKC_84909601              C      Y         N   \n",
       "219  2023        3      MNIT_84364001              C      N         N   \n",
       "\n",
       "    สรุปการผลการดำเนินงาน เหตุการณ์สำคัญ ปัจจัยต่อการเติบโต  \\\n",
       "0                       Y              Y                  Y   \n",
       "1                       N              N                  N   \n",
       "2                       Y              Y                  N   \n",
       "3                       Y              Y                  N   \n",
       "4                       Y              Y                  N   \n",
       "..                    ...            ...                ...   \n",
       "215                     N              N                  N   \n",
       "216                     Y              Y                  Y   \n",
       "217                     N              N                  N   \n",
       "218                     N              N                  N   \n",
       "219                     N              N                  N   \n",
       "\n",
       "    เปิดเผยรายได้พร้อมคำอธิบาย เปิดเผยกำไรที่เปลี่ยนไปพร้อมคำอธิบาย  \\\n",
       "0                            Y                                    Y   \n",
       "1                          NaN                                  NaN   \n",
       "2                            Y                                    Y   \n",
       "3                            Y                                    Y   \n",
       "4                            Y                                    Y   \n",
       "..                         ...                                  ...   \n",
       "215                          Y                                    Y   \n",
       "216                          Y                                    Y   \n",
       "217                          Y                                    N   \n",
       "218                        NaN                                  NaN   \n",
       "219                        NaN                                  NaN   \n",
       "\n",
       "    เปิดเผยรายได้ เปิดเผยกำไรที่เปลี่ยนไป วิเคราะห์งบหรือฐานะทางการเงิน  \\\n",
       "0             NaN                     NaN                             Y   \n",
       "1               Y                       Y                             N   \n",
       "2             NaN                     NaN                             Y   \n",
       "3             NaN                     NaN                             Y   \n",
       "4             NaN                     NaN                             Y   \n",
       "..            ...                     ...                           ...   \n",
       "215           NaN                     NaN                             N   \n",
       "216           NaN                     NaN                             Y   \n",
       "217           NaN                     NaN                             N   \n",
       "218             Y                       N                             Y   \n",
       "219             Y                       N                             N   \n",
       "\n",
       "    รายงาน ESG  \n",
       "0            Y  \n",
       "1            N  \n",
       "2            N  \n",
       "3            N  \n",
       "4            N  \n",
       "..         ...  \n",
       "215          N  \n",
       "216          Y  \n",
       "217          N  \n",
       "218          N  \n",
       "219          N  \n",
       "\n",
       "[220 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_excel(config.out_trainset)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = config.SEED  \n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Configuration Part 1 ###########\n",
    "source_dir = './'\n",
    "prefix = config.prefix\n",
    "suffix = config.suffix\n",
    "model_name = config.model_name\n",
    "loss_func_name = config.loss_func_name \n",
    "\n",
    "if model_name == 'biobert_base':\n",
    "    # model_checkpoint = os.path.join(source_dir, 'Model', 'biobert-base-cased-v1.1.bin')\n",
    "    model_checkpoint = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "    \n",
    "data_train=pickle.load(open(config.trainset,'rb'))\n",
    "\n",
    "excluded_columns = ['Year', 'Quarter', 'Filename']\n",
    "labels_ref = [col for col in df.columns if col not in excluded_columns]\n",
    "\n",
    "class_freq = []\n",
    "for label in labels_ref:\n",
    "    value_counts = df[label].value_counts(normalize=True).reindex([0, 1], fill_value=0.0)\n",
    "    label_freq = value_counts.tolist()  # Convert to a list\n",
    "    class_freq.append(label_freq)\n",
    "\n",
    "# Convert the list to a numpy array with the correct data type\n",
    "class_freq = np.array(class_freq, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Convert to a numpy array of proper type\n",
    "class_freq = np.array(class_freq, dtype=np.float32)\n",
    "\n",
    "\n",
    "train_num=len(df)\n",
    "num_labels = len(labels_ref)\n",
    "\n",
    "max_len = config.max_len\n",
    "lr = config.lr\n",
    "epochs = config.epochs\n",
    "batch_size = config.batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/thanawin/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/thanawin/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/pytorch_model.bin\n",
      "/home/thanawin/miniconda3/envs/torchenv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ########## set up ###########\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, max_len=max_len)\n",
    "# tokenizer.save_pretrained(\"./biobert-tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./biobert-tokenizer\")\n",
    "\n",
    "\n",
    "\n",
    "model_config = BertConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    config=model_config,\n",
    "    ignore_mismatched_sizes=True  \n",
    ")\n",
    "model = nn.DataParallel(model).to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = optim.AdamW(optimizer_grouped_parameters, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## Configuration Part 2 ###########\n",
    "\n",
    "if loss_func_name == 'BCE':\n",
    "    loss_func = ResampleLoss(reweight_func=None, loss_weight=1.0,\n",
    "                             focal=dict(focal=False, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(),\n",
    "                             class_freq=class_freq, train_num=train_num)\n",
    "    \n",
    "if loss_func_name == 'FL':\n",
    "    loss_func = ResampleLoss(reweight_func=None, loss_weight=1.0,\n",
    "                             focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(),\n",
    "                             class_freq=class_freq, train_num=train_num) \n",
    "    \n",
    "if loss_func_name == 'CBloss': #CB\n",
    "    loss_func = ResampleLoss(reweight_func='CB', loss_weight=5.0,\n",
    "                             focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(),\n",
    "                             CB_loss=dict(CB_beta=0.9, CB_mode='by_class'),\n",
    "                             class_freq=class_freq, train_num=train_num) \n",
    "    \n",
    "if loss_func_name == 'R-BCE-Focal': # R-FL\n",
    "    loss_func = ResampleLoss(reweight_func='rebalance', loss_weight=1.0,\n",
    "                             focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(),\n",
    "                             map_param=dict(alpha=0.1, beta=10.0, gamma=0.05), \n",
    "                             class_freq=class_freq, train_num=train_num)\n",
    "    \n",
    "if loss_func_name == 'NTR-Focal': # NTR-FL\n",
    "    loss_func = ResampleLoss(reweight_func=None, loss_weight=0.5,\n",
    "                             focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n",
    "                             class_freq=class_freq, train_num=train_num)  \n",
    "\n",
    "if loss_func_name == 'DBloss-noFocal': # DB-0FL\n",
    "    loss_func = ResampleLoss(reweight_func='rebalance', loss_weight=0.5,\n",
    "                             focal=dict(focal=False, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n",
    "                             map_param=dict(alpha=0.1, beta=10.0, gamma=0.05), \n",
    "                             class_freq=class_freq, train_num=train_num)\n",
    "    \n",
    "if loss_func_name == 'CBloss-ntr': # CB-NTR\n",
    "    loss_func = ResampleLoss(reweight_func='CB', loss_weight=10.0,\n",
    "                             focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n",
    "                             CB_loss=dict(CB_beta=0.9, CB_mode='by_class'),\n",
    "                             class_freq=class_freq, train_num=train_num) \n",
    "    \n",
    "if loss_func_name == 'DBloss': # DB\n",
    "    loss_func = ResampleLoss(reweight_func='rebalance', loss_weight=1.0,\n",
    "                             focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "                             logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n",
    "                             map_param=dict(alpha=0.1, beta=10.0, gamma=0.05), \n",
    "                             class_freq=class_freq, train_num=train_num)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/thanawin/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########## data preprocessing (one-off configuration based on the input data) ###########\n",
    "\n",
    "model_encode = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def EncodingText(model,sentence):\n",
    "     embeddings = model.encode([sentence])\n",
    "     return embeddings\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, excel_file, data_dict, target_columns=None):\n",
    "        self.data = pd.read_excel(excel_file)\n",
    "        self.target_columns = target_columns or [col for col in self.data.columns \n",
    "                                                  if col not in [\"Year\", \"Quarter\", \"Filename\"]]\n",
    "        self.text_dict = data_dict\n",
    "        \n",
    "        \n",
    "        self.label_encoders = {col: LabelEncoder().fit(self.data[col].fillna('0')) for col in self.target_columns}\n",
    "        with open(\"label_encoders.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.label_encoders, f)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "     \n",
    "        filename = row[\"Filename\"]\n",
    "        paragraph_text = self.text_dict.get(filename, None)\n",
    "\n",
    "        if not paragraph_text or not isinstance(paragraph_text, str):\n",
    "            paragraph_text = \"Unknown text\"\n",
    "        \n",
    "        text_vector = EncodingText(model_encode, paragraph_text)\n",
    "        \n",
    "        \n",
    "        targets = {}\n",
    "        for col in self.target_columns:\n",
    "            value = row[col]\n",
    "            value = '0' if pd.isna(value) else value \n",
    "            targets[col] = self.label_encoders[col].transform([value])[0] \n",
    "        targets_tensor = torch.tensor(list(targets.values()), dtype=torch.float32)\n",
    "        \n",
    "        return text_vector, targets_tensor\n",
    "\n",
    "\n",
    "    \n",
    "########## start training + val ###########\n",
    "keys = list(data_train.keys())\n",
    "train_keys, val_keys = train_test_split(keys, test_size=0.2, random_state=42)\n",
    "train_docs = {key: data_train[key] for key in train_keys}\n",
    "val_docs = {key: data_train[key] for key in val_keys}\n",
    "\n",
    "train_dataloader = DataLoader(CustomDataset(config.out_trainset,train_docs), shuffle=True, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(CustomDataset(config.out_trainset,val_docs), shuffle=False, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Text Vector (Embeddings): tensor([[[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]]])\n",
      "Targets: tensor([[2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [1., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 1., 2., 1., 0., 0., 2., 2., 0., 0., 1., 1.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [2., 1., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 1., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 1., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 2., 1., 0., 0., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 1., 2., 1., 0., 1., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [1., 1., 2., 1., 0., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.]])\n",
      "\n",
      "\n",
      "Batch 2:\n",
      "Text Vector (Embeddings): tensor([[[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]],\n",
      "\n",
      "        [[-0.2214, -0.1492, -0.0243,  ..., -0.1206, -0.0379,  0.1480]]])\n",
      "Targets: tensor([[1., 1., 1., 0., 0., 0., 0., 0., 2., 1., 1., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 0., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 1., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [2., 0., 1., 1., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 2., 2., 0., 0., 1., 1.],\n",
      "        [2., 0., 1., 1., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 2., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 2., 2., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 2., 1., 0., 0., 1., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 2., 2., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [0., 1., 2., 1., 1., 1., 2., 2., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 2., 1., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 2., 0., 0.],\n",
      "        [1., 0., 2., 0., 0., 1., 2., 2., 0., 0., 0., 0.],\n",
      "        [2., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 0.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (text_vector, targets) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i + 1}:\")\n",
    "    print(\"Text Vector (Embeddings):\", text_vector)\n",
    "    print(\"Targets:\", targets)\n",
    "    print(\"\\n\")\n",
    "    if i >= 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_f1_for_epoch = 0\n",
    "# epochs_without_improvement = 0\n",
    "\n",
    "# for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "#     # Training\n",
    "#     model.train()\n",
    "#     tr_loss = 0\n",
    "#     nb_tr_steps = 0\n",
    "  \n",
    "#     for _, batch in enumerate(train_dataloader):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "#         logits = outputs[0]\n",
    "#         loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         tr_loss += loss.item()\n",
    "#         nb_tr_steps += 1\n",
    "\n",
    "#     print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     nb_val_steps = 0\n",
    "#     true_labels,pred_labels = [],[]\n",
    "    \n",
    "#     for _, batch in enumerate(validation_dataloader):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         with torch.no_grad():\n",
    "#             # Forward pass\n",
    "#             outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "#             b_logit_pred = outs[0]\n",
    "#             pred_label = torch.sigmoid(b_logit_pred)\n",
    "#             loss = loss_func(b_logit_pred.view(-1,num_labels),b_labels.type_as(b_logit_pred).view(-1,num_labels))\n",
    "#             val_loss += loss.item()\n",
    "#             nb_val_steps += 1\n",
    "    \n",
    "#             b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "#             pred_label = pred_label.to('cpu').numpy()\n",
    "#             b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "#         true_labels.append(b_labels)\n",
    "#         pred_labels.append(pred_label)\n",
    "    \n",
    "#     print(\"Validation loss: {}\".format(val_loss/nb_val_steps))\n",
    "\n",
    "#     # Flatten outputs\n",
    "#     true_labels = [item for sublist in true_labels for item in sublist]\n",
    "#     pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "\n",
    "#     # Calculate Accuracy\n",
    "#     threshold = 0.5\n",
    "#     true_bools = [tl==1 for tl in true_labels]\n",
    "#     pred_bools = [pl>threshold for pl in pred_labels]\n",
    "#     val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "#     val_precision_accuracy = precision_score(true_bools, pred_bools,average='micro')\n",
    "#     val_recall_accuracy = recall_score(true_bools, pred_bools,average='micro')\n",
    "    \n",
    "#     print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "#     print('Precision Validation Accuracy: ', val_precision_accuracy)\n",
    "#     print('Recall Validation Accuracy: ', val_recall_accuracy)\n",
    "\n",
    "#     # Calculate AUC as well\n",
    "#     val_auc_score = roc_auc_score(true_bools, pred_labels, average='micro')\n",
    "#     print('AUC Validation: ', val_auc_score)\n",
    "    \n",
    "#     # Search best threshold for F1\n",
    "#     best_med_th = 0.5\n",
    "#     micro_thresholds = (np.array(range(-10,11))/100)+best_med_th\n",
    "#     f1_results, prec_results, recall_results = [], [], []\n",
    "#     for th in micro_thresholds:\n",
    "#         pred_bools = [pl>th for pl in pred_labels]\n",
    "#         test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "#         test_precision_accuracy = precision_score(true_bools, pred_bools,average='micro')\n",
    "#         test_recall_accuracy = recall_score(true_bools, pred_bools,average='micro')\n",
    "#         f1_results.append(test_f1_accuracy)\n",
    "#         prec_results.append(test_precision_accuracy)\n",
    "#         recall_results.append(test_recall_accuracy)\n",
    "\n",
    "#     best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "#     # Print and save classification report\n",
    "#     print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "#     print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "\n",
    "#     # Save the model if this epoch gives the best f1 score in validation set\n",
    "#     if f1_results[best_f1_idx] > (best_f1_for_epoch * 0.995):\n",
    "#         best_f1_for_epoch = f1_results[best_f1_idx]\n",
    "#         epochs_without_improvement = 0\n",
    "#         model_dir = os.path.join(source_dir, 'models')\n",
    "#         for fname in os.listdir(model_dir):\n",
    "#             if fname.startswith('_'.join([prefix,model_name,loss_func_name,suffix])):\n",
    "#                 os.remove(os.path.join(model_dir, fname))\n",
    "#         torch.save(model.state_dict(), os.path.join(model_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'para'))\n",
    "#     else:\n",
    "#         epochs_without_improvement += 1\n",
    "    \n",
    "#     log_dir = os.path.join(source_dir, 'logs')\n",
    "#     # Log all results in validation set with different thresholds\n",
    "#     with open(os.path.join(log_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'.json'),'w') as f:\n",
    "#         d = {}\n",
    "#         d[\"f1_accuracy_default\"] =  val_f1_accuracy\n",
    "#         d[\"pr_accuracy_default\"] =  val_precision_accuracy\n",
    "#         d[\"rec_accuracy_default\"] =  val_recall_accuracy\n",
    "#         d[\"auc_score_default\"] =  val_auc_score\n",
    "#         d[\"thresholds\"] =  list(micro_thresholds)\n",
    "#         d[\"threshold_f1s\"] =  f1_results\n",
    "#         d[\"threshold_precs\"] =  prec_results\n",
    "#         d[\"threshold_recalls\"] =  recall_results\n",
    "#         json.dump(d, f)\n",
    "    \n",
    "#     open(os.path.join(log_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'.tmp'),'w').write('%s %s' % (micro_thresholds[best_f1_idx], f1_results[best_f1_idx]))\n",
    "\n",
    "#     # If 5 epochs pass without improvement consider the model as saturated and exit\n",
    "#     if epochs_without_improvement > 4:\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
